# -*- coding: utf-8 -*-
"""LSSDP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c0L1ag-Pj_R6tpDOJURgz69rYSFNAwrz
"""

# ============================================
# ðŸ“Œ NLP Phases for Fake vs Real Detection
# Using Naive Bayes at each step
# With Robust Preprocessing (Fixed Pragmatic Phase)
# ============================================

# âœ… Install dependencies (uncomment if running in Colab)
# !pip install -q scikit-learn pandas nltk spacy textblob
# !python -m spacy download en_core_web_sm

# -------------------------
# Import libraries
# -------------------------
import pandas as pd
import numpy as np
import nltk, spacy
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize, sent_tokenize
from textblob import TextBlob

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# -------------------------
# Download NLTK resources
# -------------------------
nltk.download("punkt")
nltk.download("wordnet")
nltk.download("stopwords")
nltk.download("punkt_tab")   # Fix for newer NLTK tokenizers

# -------------------------
# Load spaCy model
# -------------------------
nlp = spacy.load("en_core_web_sm")

# ============================
# Step 1: Load Dataset
# ============================
# For Google Colab
from google.colab import files
uploaded = files.upload()
df = pd.read_csv(list(uploaded.keys())[0])

print("Dataset Shape:", df.shape)
print("Columns:", df.columns)
display(df.head())

# Use the correct column names
X = df["Statement"]
y = df["BinaryTarget"]

# ============================
# Step 2: Robust Preprocessing
# ============================
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

def robust_preprocess(text):
    """Lowercase, remove non-alpha, remove stopwords, lemmatize"""
    if pd.isnull(text):
        return ""
    text = str(text).lower()
    tokens = word_tokenize(text)
    tokens = [t for t in tokens if t.isalpha()]  # keep only alphabetic
    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]
    return " ".join(tokens)

# Apply preprocessing
X_processed = X.astype(str).apply(robust_preprocess)

# Drop rows that became empty after preprocessing
mask_global = X_processed.str.strip().astype(bool)
X_processed, y_filtered = X_processed[mask_global], y[mask_global]

# ============================
# Helper: Train Naive Bayes
# ============================
def train_nb(X_features, y, name):
    X_train, X_test, y_train, y_test = train_test_split(
        X_features, y, test_size=0.2, random_state=42
    )
    model = MultinomialNB()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"\nðŸ”¹ {name} Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred, zero_division=0))
    return acc

# ============================
# Phase 1: Lexical & Morphological Analysis
# ============================
X_lexical = X_processed.copy()
mask_lex = X_lexical.str.strip().astype(bool)
X_lexical, y_lex = X_lexical[mask_lex], y_filtered[mask_lex]

if X_lexical.empty:
    print("\nðŸ”¹ Lexical & Morphological Analysis: No documents left after filtering.")
    acc1 = None
else:
    vec_lexical = CountVectorizer().fit_transform(X_lexical)
    acc1 = train_nb(vec_lexical, y_lex, "Lexical & Morphological Analysis")

# ============================
# Phase 2: Syntactic Analysis
# ============================
def syntactic_features(text):
    doc = nlp(text)
    pos_tags = " ".join([token.pos_ for token in doc])
    return pos_tags

X_syntax = X_processed.apply(syntactic_features)
mask_syn = X_syntax.str.strip().astype(bool)
X_syntax, y_syn = X_syntax[mask_syn], y_filtered[mask_syn]

if X_syntax.empty:
    print("\nðŸ”¹ Syntactic Analysis: No documents left after filtering.")
    acc2 = None
else:
    vec_syntax = CountVectorizer().fit_transform(X_syntax)
    acc2 = train_nb(vec_syntax, y_syn, "Syntactic Analysis")

# ============================
# Phase 3: Semantic Analysis
# ============================
def semantic_features(text):
    blob = TextBlob(text)
    return f"{blob.sentiment.polarity} {blob.sentiment.subjectivity}"

X_semantic = X_processed.apply(semantic_features)
mask_sem = X_semantic.str.strip().astype(bool)
X_semantic, y_sem = X_semantic[mask_sem], y_filtered[mask_sem]

if X_semantic.empty:
    print("\nðŸ”¹ Semantic Analysis: No documents left after filtering.")
    acc3 = None
else:
    vec_semantic = TfidfVectorizer().fit_transform(X_semantic)
    acc3 = train_nb(vec_semantic, y_sem, "Semantic Analysis")

# ============================
# Phase 4: Discourse Integration
# ============================
def discourse_features(text):
    sentences = sent_tokenize(text)
    if len(sentences) == 0:
        return "0"
    return f"{len(sentences)} {' '.join([s.split()[0] for s in sentences if len(s.split())>0])}"

X_discourse = X_processed.apply(discourse_features)
mask_disc = X_discourse.str.strip().astype(bool)
X_discourse, y_disc = X_discourse[mask_disc], y_filtered[mask_disc]

if X_discourse.empty:
    print("\nðŸ”¹ Discourse Integration: No documents left after filtering.")
    acc4 = None
else:
    vec_discourse = CountVectorizer().fit_transform(X_discourse)
    acc4 = train_nb(vec_discourse, y_disc, "Discourse Integration")

# ============================
# Phase 5: Pragmatic Analysis (FIXED)
# ============================
pragmatic_words = ["must", "should", "might", "could", "will", "?", "!"]

def pragmatic_features(text):
    text_lower = text.lower()
    counts = [text_lower.count(w) for w in pragmatic_words]
    return counts  # return list of integers directly

# Create a numeric matrix (each row is a list of counts)
pragmatic_matrix = np.array([pragmatic_features(t) for t in X_processed])
mask_prag = pragmatic_matrix.sum(axis=1) >= 0   # keep all rows
X_pragmatic_matrix, y_prag = pragmatic_matrix[mask_prag], y_filtered[mask_prag]

if X_pragmatic_matrix.shape[0] == 0:
    print("\nðŸ”¹ Pragmatic Analysis: No documents left after filtering.")
    acc5 = None
else:
    acc5 = train_nb(X_pragmatic_matrix, y_prag, "Pragmatic Analysis")

# ============================
# Final Results
# ============================
print("\nðŸ“Š Phase-wise Naive Bayes Accuracies:")
def format_acc(a): return f"{a:.4f}" if a is not None else "N/A"
print(f"1. Lexical & Morphological: {format_acc(acc1)}")
print(f"2. Syntactic: {format_acc(acc2)}")
print(f"3. Semantic: {format_acc(acc3)}")
print(f"4. Discourse: {format_acc(acc4)}")
print(f"5. Pragmatic: {format_acc(acc5)}")

