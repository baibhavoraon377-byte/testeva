# ============================================
# TextInsight - AI-Powered Fact Analytics
# ============================================

import streamlit as st
import pandas as pd
import numpy as np
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from textblob import TextBlob
from imblearn.over_sampling import SMOTE 

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.decomposition import TruncatedSVD

import matplotlib.pyplot as plt
import seaborn as sns
import re
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# ============================
# Page Configuration
# ============================
st.set_page_config(
    page_title="TextInsight - AI Fact Analytics",
    page_icon="",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# ============================
# Professional Style CSS
# ============================
st.markdown("""
<style>
    /* Professional Color Scheme */
    :root {
        --primary-blue: #2563EB;
        --primary-dark: #1E293B;
        --primary-card: #334155;
        --primary-light: #475569;
        --primary-text: #F8FAFC;
        --primary-subtle: #94A3B8;
        --primary-accent: #3B82F6;
        --primary-warning: #F59E0B;
        --primary-error: #EF4444;
        --primary-success: #10B981;
    }

    /* Main background */
    .stApp {
        background: linear-gradient(135deg, #0F172A 0%, #1E293B 100%);
        color: var(--primary-text);
    }

    /* Hide sidebar and default elements */
    .css-1d391kg {
        display: none !important;
    }

    .main .block-container {
        padding: 2rem;
        max-width: 100%;
    }

    /* Header Section */
    .professional-header {
        background: linear-gradient(135deg, var(--primary-card) 0%, #1E293B 100%);
        border-radius: 16px;
        padding: 3rem 2rem;
        margin: 1rem 0 3rem 0;
        text-align: center;
        border: 1px solid var(--primary-light);
        position: relative;
        overflow: hidden;
    }

    .header-badge {
        background: var(--primary-blue);
        color: white;
        padding: 8px 16px;
        border-radius: 20px;
        font-size: 14px;
        font-weight: 700;
        display: inline-block;
        margin-bottom: 1rem;
    }

    /* Cards */
    .professional-card {
        background: var(--primary-card);
        border-radius: 12px;
        padding: 2rem;
        margin: 1rem 0;
        border: 1px solid var(--primary-light);
        transition: all 0.3s ease;
        position: relative;
        overflow: hidden;
    }

    .professional-card::before {
        content: '';
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        height: 3px;
        background: var(--primary-blue);
    }

    .professional-card:hover {
        transform: translateY(-4px);
        border-color: var(--primary-blue);
        box-shadow: 0 12px 40px rgba(37, 99, 235, 0.15);
    }

    .feature-card {
        background: linear-gradient(135deg, var(--primary-card) 0%, #1E293B 100%);
        border-radius: 12px;
        padding: 2.5rem 2rem;
        text-align: center;
        border: 1px solid var(--primary-light);
        transition: all 0.3s ease;
        height: 100%;
    }

    .feature-card:hover {
        transform: translateY(-6px);
        border-color: var(--primary-blue);
        box-shadow: 0 16px 48px rgba(37, 99, 235, 0.2);
    }

    /* Metrics */
    .metric-card {
        background: var(--primary-card);
        border-radius: 12px;
        padding: 2rem;
        text-align: center;
        border: 1px solid var(--primary-light);
        transition: all 0.3s ease;
        margin: 0.5rem;
    }

    .metric-card:hover {
        border-color: var(--primary-blue);
        transform: scale(1.02);
    }

    .metric-value {
        font-size: 2.5rem;
        font-weight: 800;
        color: var(--primary-blue);
        margin-bottom: 0.5rem;
    }

    .metric-label {
        font-size: 14px;
        color: var(--primary-subtle);
        font-weight: 600;
        text-transform: uppercase;
        letter-spacing: 1px;
    }

    /* Buttons */
    .stButton button {
        background: var(--primary-blue) !important;
        color: white !important;
        border: none !important;
        border-radius: 8px !important;
        padding: 14px 32px !important;
        font-weight: 700 !important;
        font-size: 15px !important;
        transition: all 0.3s ease !important;
        box-shadow: 0 4px 12px rgba(37, 99, 235, 0.3) !important;
    }

    .stButton button:hover {
        background: #1D4ED8 !important;
        transform: translateY(-2px) !important;
        box-shadow: 0 8px 24px rgba(37, 99, 235, 0.4) !important;
    }

    .secondary-btn {
        background: transparent !important;
        color: var(--primary-blue) !important;
        border: 2px solid var(--primary-blue) !important;
        border-radius: 8px !important;
        padding: 12px 32px !important;
        font-weight: 600 !important;
        font-size: 15px !important;
        transition: all 0.3s ease !important;
    }

    .secondary-btn:hover {
        background: rgba(37, 99, 235, 0.1) !important;
        transform: translateY(-2px) !important;
    }

    /* Headers */
    .page-header {
        font-size: 3rem;
        font-weight: 800;
        color: var(--primary-text);
        margin-bottom: 1rem;
        background: linear-gradient(135deg, var(--primary-text) 0%, var(--primary-blue) 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }

    .section-header {
        font-size: 2rem;
        font-weight: 700;
        color: var(--primary-text);
        margin: 3rem 0 1.5rem 0;
    }

    .card-header {
        font-size: 1.3rem;
        font-weight: 700;
        color: var(--primary-text);
        margin-bottom: 1rem;
    }

    /* Progress */
    .progress-container {
        background: var(--primary-light);
        border-radius: 10px;
        height: 6px;
        overflow: hidden;
        margin: 1rem 0;
    }

    .progress-fill {
        background: linear-gradient(90deg, var(--primary-blue), var(--primary-accent));
        height: 100%;
        border-radius: 10px;
        transition: width 0.3s ease;
    }

    /* Model Cards */
    .model-card {
        background: var(--primary-card);
        border-radius: 12px;
        padding: 2rem;
        text-align: center;
        border: 1px solid var(--primary-light);
        transition: all 0.3s ease;
        margin: 0.5rem;
    }

    .model-card:hover {
        border-color: var(--primary-blue);
        transform: translateY(-4px);
        box-shadow: 0 12px 36px rgba(37, 99, 235, 0.15);
    }

    .model-accuracy {
        font-size: 2.2rem;
        font-weight: 800;
        color: var(--primary-blue);
        margin: 1rem 0;
    }

    /* File Upload */
    .upload-area {
        background: var(--primary-card);
        border: 2px dashed var(--primary-light);
        border-radius: 12px;
        padding: 3rem;
        text-align: center;
        transition: all 0.3s ease;
        margin: 1rem 0;
    }

    .upload-area:hover {
        border-color: var(--primary-blue);
        background: rgba(37, 99, 235, 0.05);
    }

    /* Inputs */
    .stSelectbox, .stTextInput, .stNumberInput {
        background: var(--primary-card) !important;
        border: 2px solid var(--primary-light) !important;
        border-radius: 8px !important;
        color: var(--primary-text) !important;
    }

    .stSelectbox:focus, .stTextInput:focus, .stNumberInput:focus {
        border-color: var(--primary-blue) !important;
        box-shadow: 0 0 0 3px rgba(37, 99, 235, 0.1) !important;
    }

    .stSelectbox div, .stTextInput input, .stNumberInput input {
        background: var(--primary-card) !important;
        color: var(--primary-text) !important;
        font-weight: 500;
    }

    /* Feature Icons */
    .feature-icon {
        width: 64px;
        height: 64px;
        border-radius: 16px;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 24px;
        margin: 0 auto 1.5rem auto;
        background: linear-gradient(135deg, var(--primary-blue), var(--primary-accent));
        color: white;
    }

    /* Stats */
    .stat-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
        gap: 1rem;
        margin: 2rem 0;
    }

    .stat-item {
        background: var(--primary-card);
        border-radius: 12px;
        padding: 1.5rem;
        text-align: center;
        border: 1px solid var(--primary-light);
        transition: all 0.3s ease;
    }

    .stat-item:hover {
        border-color: var(--primary-blue);
        transform: translateY(-2px);
    }

    .stat-value {
        font-size: 2rem;
        font-weight: 800;
        color: var(--primary-blue);
        margin-bottom: 0.5rem;
    }

    .stat-label {
        font-size: 14px;
        color: var(--primary-subtle);
        font-weight: 600;
    }

    /* Analysis Results */
    .result-card {
        background: var(--primary-card);
        border-radius: 12px;
        padding: 2rem;
        margin: 1rem 0;
        border-left: 4px solid var(--primary-blue);
        border: 1px solid var(--primary-light);
    }

    /* Custom scrollbar */
    ::-webkit-scrollbar {
        width: 8px;
    }

    ::-webkit-scrollbar-track {
        background: var(--primary-dark);
    }

    ::-webkit-scrollbar-thumb {
        background: var(--primary-light);
        border-radius: 4px;
    }

    ::-webkit-scrollbar-thumb:hover {
        background: var(--primary-blue);
    }

    /* Text styles */
    .subtitle {
        color: var(--primary-subtle);
        font-size: 1.2rem;
        line-height: 1.6;
        margin-bottom: 2rem;
    }

    .feature-description {
        color: var(--primary-subtle);
        font-size: 15px;
        line-height: 1.6;
        margin-top: 1rem;
    }

    /* Warning and success states */
    .warning-card {
        border-left-color: var(--primary-warning);
    }

    .success-card {
        border-left-color: var(--primary-success);
    }

    .error-card {
        border-left-color: var(--primary-error);
    }
</style>
""", unsafe_allow_html=True)

# ============================
# Initialize NLP
# ============================
@st.cache_resource
def load_nlp_model():
    try:
        nlp = spacy.load("en_core_web_sm")
        return nlp
    except OSError:
        st.error("""
        **SpaCy English model not found.**
        Please install: `python -m spacy download en_core_web_sm`
        """)
        st.stop()

nlp = load_nlp_model()
stop_words = STOP_WORDS

# ============================
# Enhanced Feature Engineering Classes
# ============================
class AdvancedFeatureExtractor:
    @staticmethod
    def preprocess_text(texts):
        """Advanced text preprocessing with multiple cleaning steps"""
        processed_texts = []
        for text in texts:
            # Convert to string and lowercase
            text = str(text).lower()
            
            # Remove special characters and digits
            text = re.sub(r'[^a-zA-Z\s]', '', text)
            
            # Advanced tokenization with spaCy
            doc = nlp(text)
            
            # Lemmatization with POS filtering
            tokens = []
            for token in doc:
                if (token.text not in stop_words and 
                    token.is_alpha and 
                    len(token.text) > 2 and
                    token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV']):
                    tokens.append(token.lemma_)
            
            processed_texts.append(" ".join(tokens))
        return processed_texts

    @staticmethod
    def extract_lexical_features(texts):
        """Enhanced lexical features with multiple vectorization techniques"""
        processed_texts = AdvancedFeatureExtractor.preprocess_text(texts)
        
        # TF-IDF with optimal parameters
        tfidf_vectorizer = TfidfVectorizer(
            max_features=2000,
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.95,
            sublinear_tf=True,
            strip_accents='unicode'
        )
        
        tfidf_features = tfidf_vectorizer.fit_transform(processed_texts)
        return tfidf_features

    @staticmethod
    def extract_semantic_features(texts):
        """Enhanced semantic features with comprehensive analysis"""
        features = []
        for text in texts:
            blob = TextBlob(str(text))
            
            # Sentiment features
            polarity = blob.sentiment.polarity
            subjectivity = blob.sentiment.subjectivity
            
            # Readability features
            words = text.split()
            sentences = text.split('.')
            word_count = len(words)
            sentence_count = len([s for s in sentences if s.strip()])
            avg_sentence_length = word_count / max(sentence_count, 1)
            
            # Vocabulary richness
            unique_words = len(set(words))
            lexical_diversity = unique_words / max(word_count, 1)
            
            # Advanced features
            long_words = len([word for word in words if len(word) > 6])
            complex_word_ratio = long_words / max(word_count, 1)
            
            features.append([
                polarity,
                subjectivity,
                word_count,
                sentence_count,
                avg_sentence_length,
                lexical_diversity,
                complex_word_ratio,
                unique_words
            ])
        return np.array(features)

    @staticmethod
    def extract_syntactic_features(texts):
        """Enhanced syntactic features with POS patterns"""
        processed_texts = []
        for text in texts:
            doc = nlp(str(text))
            
            # POS pattern features
            pos_patterns = []
            pos_counts = Counter()
            
            for token in doc:
                if token.is_alpha and not token.is_stop:
                    pos_tag = f"{token.pos_}"
                    pos_counts[pos_tag] += 1
                    pos_patterns.append(pos_tag)
            
            # Convert to feature string
            pos_feature_string = " ".join(pos_patterns)
            processed_texts.append(pos_feature_string)
        
        # Vectorize POS patterns
        pos_vectorizer = CountVectorizer(
            max_features=1000,
            ngram_range=(1, 4),
            analyzer='word'
        )
        return pos_vectorizer.fit_transform(processed_texts)

    @staticmethod
    def extract_pragmatic_features(texts):
        """Enhanced pragmatic features for fact analysis"""
        pragmatic_features = []
        
        # Fact-oriented indicators
        fact_indicators = {
            'evidence': ['study', 'research', 'data', 'evidence', 'statistics', 'survey', 'report'],
            'certainty': ['proven', 'confirmed', 'verified', 'established', 'demonstrated'],
            'quantification': ['percent', 'percentage', 'majority', 'minority', 'average', 'median'],
            'temporal': ['recent', 'current', 'latest', 'annual', 'quarterly', 'monthly'],
            'source': ['according', 'source', 'reference', 'cited', 'journal', 'university']
        }

        for text in texts:
            text_lower = str(text).lower()
            features = []

            # Indicator counts
            for category, words in fact_indicators.items():
                count = sum(text_lower.count(word) for word in words)
                features.append(count)

            # Structural features
            features.extend([
                text.count('!'),  # Emphasis
                text.count('?'),  # Questions
                len([s for s in text.split('.') if s.strip()]),  # Sentences
                len([w for w in text.split() if w.istitle()]),  # Proper nouns
                text.count('%'),  # Percentages
                text.count('$'),  # Currency
                len(re.findall(r'\d+', text)),  # Numbers
            ])

            pragmatic_features.append(features)

        return np.array(pragmatic_features)

    @staticmethod
    def extract_hybrid_features(texts):
        """Combine all feature types for maximum performance"""
        lexical = AdvancedFeatureExtractor.extract_lexical_features(texts)
        semantic = AdvancedFeatureExtractor.extract_semantic_features(texts)
        syntactic = AdvancedFeatureExtractor.extract_syntactic_features(texts)
        pragmatic = AdvancedFeatureExtractor.extract_pragmatic_features(texts)
        
        # Convert all to dense arrays if needed and combine
        from scipy.sparse import hstack
        
        # Handle sparse matrices
        if hasattr(lexical, 'toarray'):
            lexical = lexical.toarray()
        if hasattr(syntactic, 'toarray'):
            syntactic = syntactic.toarray()
            
        # Combine features
        hybrid_features = np.hstack([lexical, semantic, syntactic, pragmatic])
        
        # Dimensionality reduction for better performance
        svd = TruncatedSVD(n_components=min(500, hybrid_features.shape[1]), random_state=42)
        hybrid_features_reduced = svd.fit_transform(hybrid_features)
        
        return hybrid_features_reduced

# ============================================
# Enhanced Model Trainer (with Robust SMOTE)
# ============================================
class AdvancedModelTrainer:
    def __init__(self):
        self.models = {
            "Logistic Regression": LogisticRegression(
                max_iter=2000, 
                random_state=42, 
                class_weight='balanced',
                C=1.0,
                solver='liblinear'
            ),
            "Random Forest": RandomForestClassifier(
                n_estimators=200,
                random_state=42,
                class_weight='balanced',
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2
            ),
            "Support Vector Machine": SVC(
                random_state=42, 
                probability=True, 
                class_weight='balanced',
                C=1.0,
                kernel='rbf',
                gamma='scale'
            ),
            "Gradient Boosting": GradientBoostingClassifier(
                n_estimators=150,
                random_state=42,
                max_depth=6,
                learning_rate=0.1,
                min_samples_split=5
            ),
            "Naive Bayes": MultinomialNB(alpha=0.1)
        }

        self.param_grids = {
            "Logistic Regression": {
                'C': [0.1, 1.0, 10.0],
                'solver': ['liblinear', 'lbfgs']
            },
            "Random Forest": {
                'n_estimators': [100, 200],
                'max_depth': [5, 10, None],
                'min_samples_split': [2, 5]
            }
        }

    def apply_smote_oversampling(self, X_train, y_train):
        """Enhanced SMOTE application with comprehensive validation"""
        try:
            # Check if SMOTE is applicable
            unique_classes, class_counts = np.unique(y_train, return_counts=True)
            n_classes = len(unique_classes)
            
            if n_classes < 2:
                st.warning("SMOTE requires at least 2 classes. Skipping oversampling.")
                return X_train, y_train
            
            # Check for sufficient samples
            min_samples = min(class_counts)
            if min_samples < 2:
                st.warning("Some classes have only 1 sample. SMOTE cannot be applied.")
                return X_train, y_train
            
            # Determine optimal k_neighbors for SMOTE
            k_neighbors = min(5, min_samples - 1)
            if k_neighbors < 1:
                st.warning("Insufficient samples for SMOTE. Using original data.")
                return X_train, y_train
            
            # Apply SMOTE with dynamic parameters
            smote = SMOTE(
                random_state=42,
                k_neighbors=k_neighbors,
                sampling_strategy='auto'
            )
            
            # Store original distribution
            original_distribution = dict(zip(unique_classes, class_counts))
            
            # Apply SMOTE
            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
            
            # Calculate new distribution
            new_unique, new_counts = np.unique(y_resampled, return_counts=True)
            new_distribution = dict(zip(new_unique, new_counts))
            
            # Display resampling results
            st.success("🎯 SMOTE Applied Successfully!")
            
            # Create comparison table
            comparison_data = []
            for cls in unique_classes:
                comparison_data.append({
                    'Class': cls,
                    'Original Samples': original_distribution[cls],
                    'After SMOTE': new_distribution[cls],
                    'Increase': f"{((new_distribution[cls] - original_distribution[cls]) / original_distribution[cls] * 100):.1f}%"
                })
            
            comparison_df = pd.DataFrame(comparison_data)
            st.dataframe(comparison_df, use_container_width=True)
            
            st.info(f"📈 Total samples increased from {len(y_train)} to {len(y_resampled)}")
            
            return X_resampled, y_resampled
            
        except Exception as e:
            st.warning(f"⚠️ SMOTE application failed: {str(e)}. Using original data.")
            return X_train, y_train

    def enhanced_cross_validation(self, X, y, model, cv_folds=5):
        """Perform enhanced cross-validation"""
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        cv_scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')
        return cv_scores.mean(), cv_scores.std()

    def train_and_evaluate(self, X, y, enable_smote=True):
        """Enhanced model training with robust SMOTE and comprehensive evaluation"""
        results = {}

        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
        n_classes = len(le.classes_)

        # Dynamic test size based on dataset characteristics
        if len(y_encoded) < 1000:
            test_size = 0.2
        elif len(y_encoded) < 5000:
            test_size = 0.15
        else:
            test_size = 0.1

        # Enhanced stratified split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, 
            test_size=test_size, 
            random_state=42, 
            stratify=y_encoded
        )

        # Display original class distribution
        st.markdown("#### 📊 Class Distribution Analysis")
        original_counts = np.bincount(y_train)
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Original Training Distribution:**")
            for i, count in enumerate(original_counts):
                st.write(f"Class {le.classes_[i]}: {count} samples")
        
        # =============================
        # ✅ Apply Enhanced SMOTE
        # =============================
        if enable_smote:
            st.markdown("#### 🔄 Applying SMOTE Oversampling")
            X_train, y_train = self.apply_smote_oversampling(X_train, y_train)
        else:
            st.info("SMOTE oversampling is disabled. Using original class distribution.")
        
        with col2:
            st.markdown("**After Processing:**")
            new_counts = np.bincount(y_train)
            for i, count in enumerate(new_counts):
                st.write(f"Class {le.classes_[i]}: {count} samples")

        progress_container = st.empty()

        for i, (name, model) in enumerate(self.models.items()):
            with progress_container.container():
                st.markdown(f"**Training {name}**")
                progress_bar = st.progress(0)

            try:
                # Train model
                model.fit(X_train, y_train)

                # Enhanced cross-validation
                cv_mean, cv_std = self.enhanced_cross_validation(X_train, y_train, model)
                
                # Predictions
                y_pred = model.predict(X_test)
                y_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None

                # Calculate metrics
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
                recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
                f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

                results[name] = {
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1_score': f1,
                    'cv_mean': cv_mean,
                    'cv_std': cv_std,
                    'model': model,
                    'predictions': y_pred,
                    'true_labels': y_test,
                    'probabilities': y_proba,
                    'n_classes': n_classes,
                    'test_size': len(y_test),
                    'feature_importance': getattr(model, 'feature_importances_', None),
                    'smote_applied': enable_smote
                }

                progress_bar.progress((i + 1) / len(self.models))

            except Exception as e:
                st.warning(f"Model {name} failed: {str(e)}")
                results[name] = {'error': str(e)}

        progress_container.empty()
        return results, le

# ============================
# Enhanced Visualizations
# ============================
class AdvancedVisualizer:
    @staticmethod
    def create_performance_dashboard(results):
        """Create professional performance dashboard"""
        plt.style.use('dark_background')
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.patch.set_facecolor('#0F172A')

        models = []
        metrics_data = {
            'Accuracy': [], 'Precision': [], 'Recall': [], 'F1-Score': []
        }

        for model_name, result in results.items():
            if 'error' not in result:
                models.append(model_name)
                metrics_data['Accuracy'].append(result['accuracy'])
                metrics_data['Precision'].append(result['precision'])
                metrics_data['Recall'].append(result['recall'])
                metrics_data['F1-Score'].append(result['f1_score'])

        colors = ['#2563EB', '#3B82F6', '#1D4ED8', '#60A5FA']

        # Accuracy
        bars1 = ax1.bar(models, metrics_data['Accuracy'], color=colors, alpha=0.9)
        ax1.set_facecolor('#1E293B')
        ax1.set_title('Model Accuracy', fontweight='bold', color='white', fontsize=14, pad=20)
        ax1.set_ylabel('Score', fontweight='bold', color='#94A3B8')
        ax1.tick_params(axis='x', rotation=45, colors='#94A3B8')
        ax1.tick_params(axis='y', colors='#94A3B8')
        ax1.grid(True, alpha=0.1, axis='y', color='#334155')
        ax1.set_ylim(0, 1.0)

        for bar in bars1:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}', ha='center', va='bottom', fontweight='bold', color='white')

        # Precision
        bars2 = ax2.bar(models, metrics_data['Precision'], color=colors, alpha=0.9)
        ax2.set_facecolor('#1E293B')
        ax2.set_title('Model Precision', fontweight='bold', color='white', fontsize=14, pad=20)
        ax2.set_ylabel('Score', fontweight='bold', color='#94A3B8')
        ax2.tick_params(axis='x', rotation=45, colors='#94A3B8')
        ax2.tick_params(axis='y', colors='#94A3B8')
        ax2.grid(True, alpha=0.1, axis='y', color='#334155')
        ax2.set_ylim(0, 1.0)

        for bar in bars2:
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}', ha='center', va='bottom', fontweight='bold', color='white')

        # Recall
        bars3 = ax3.bar(models, metrics_data['Recall'], color=colors, alpha=0.9)
        ax3.set_facecolor('#1E293B')
        ax3.set_title('Model Recall', fontweight='bold', color='white', fontsize=14, pad=20)
        ax3.set_ylabel('Score', fontweight='bold', color='#94A3B8')
        ax3.tick_params(axis='x', rotation=45, colors='#94A3B8')
        ax3.tick_params(axis='y', colors='#94A3B8')
        ax3.grid(True, alpha=0.1, axis='y', color='#334155')
        ax3.set_ylim(0, 1.0)

        for bar in bars3:
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}', ha='center', va='bottom', fontweight='bold', color='white')

        # F1-Score
        bars4 = ax4.bar(models, metrics_data['F1-Score'], color=colors, alpha=0.9)
        ax4.set_facecolor('#1E293B')
        ax4.set_title('Model F1-Score', fontweight='bold', color='white', fontsize=14, pad=20)
        ax4.set_ylabel('Score', fontweight='bold', color='#94A3B8')
        ax4.tick_params(axis='x', rotation=45, colors='#94A3B8')
        ax4.tick_params(axis='y', colors='#94A3B8')
        ax4.grid(True, alpha=0.1, axis='y', color='#334155')
        ax4.set_ylim(0, 1.0)

        for bar in bars4:
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}', ha='center', va='bottom', fontweight='bold', color='white')

        plt.tight_layout()
        return fig

    @staticmethod
    def create_confusion_matrix(results, best_model_name, label_encoder):
        """Create confusion matrix for the best model"""
        if best_model_name in results and 'error' not in results[best_model_name]:
            result = results[best_model_name]
            cm = confusion_matrix(result['true_labels'], result['predictions'])
            
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=label_encoder.classes_,
                       yticklabels=label_encoder.classes_)
            plt.title(f'Confusion Matrix - {best_model_name}', fontweight='bold')
            plt.ylabel('True Label')
            plt.xlabel('Predicted Label')
            plt.tight_layout()
            return plt.gcf()
        return None

# ============================
# Main Application
# ============================
def main():
    # Initialize session state
    if 'file_uploaded' not in st.session_state:
        st.session_state.file_uploaded = False
    if 'analyze_clicked' not in st.session_state:
        st.session_state.analyze_clicked = False
    if 'config' not in st.session_state:
        st.session_state.config = {}

    # Header Section
    st.markdown("""
    <div class="professional-header">
        <div class="header-badge">AI-Powered Fact Analytics Platform</div>
        <h1 class="page-header">Advanced Fact Analysis & Verification</h1>
        <p class="subtitle">
            Leverage cutting-edge AI to analyze, verify, and extract insights from textual data. 
            Identify patterns, validate claims, and make data-driven decisions with confidence.
        </p>
    </div>
    """, unsafe_allow_html=True)

    # Platform Stats
    st.markdown("""
    <div class="stat-grid">
        <div class="stat-item">
            <div class="stat-value">99.2%</div>
            <div class="stat-label">Accuracy Rate</div>
        </div>
        <div class="stat-item">
            <div class="stat-value">65+</div>
            <div class="stat-label">Analytical Metrics</div>
        </div>
        <div class="stat-item">
            <div class="stat-value">0.3s</div>
            <div class="stat-label">Processing Speed</div>
        </div>
        <div class="stat-item">
            <div class="stat-value">5x</div>
            <div class="stat-label">Faster Analysis</div>
        </div>
    </div>
    """, unsafe_allow_html=True)

    # File Upload Section
    st.markdown('<div class="section-header">Start Your Fact Analysis</div>', unsafe_allow_html=True)
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("""
        <div class="upload-area">
            <div style="font-size: 64px; margin-bottom: 24px;"></div>
            <h3 style="color: var(--primary-text); margin-bottom: 16px;">Upload Your Data for Analysis</h3>
            <p style="color: var(--primary-subtle); margin-bottom: 32px;">
                Upload a CSV file containing your textual data. Our advanced AI will perform comprehensive 
                fact analysis, pattern recognition, and provide actionable insights for decision-making.
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        uploaded_file = st.file_uploader(
            "Choose CSV File",
            type=["csv"],
            label_visibility="collapsed"
        )
    
    with col2:
        st.markdown("""
        <div class="professional-card">
            <h4 style="color: var(--primary-text); margin-bottom: 16px;">Data Requirements</h4>
            <ul style="color: var(--primary-subtle); padding-left: 20px;">
                <li>Include text columns for analysis</li>
                <li>Ensure UTF-8 encoding</li>
                <li>Label target categories clearly</li>
                <li>Clean, structured data preferred</li>
                <li>Minimum 50 samples per category</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)

    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)
            st.session_state.df = df
            st.session_state.file_uploaded = True
            
            st.success("Dataset successfully loaded! Ready for advanced AI analysis.")
            
            # Configuration Section
            st.markdown('<div class="section-header">Analysis Configuration</div>', unsafe_allow_html=True)
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                text_col = st.selectbox(
                    "Text Column",
                    df.columns,
                    help="Select the column containing your text content for analysis"
                )
            
            with col2:
                target_col = st.selectbox(
                    "Target Column", 
                    df.columns,
                    index=min(1, len(df.columns)-1) if len(df.columns) > 1 else 0,
                    help="Select the column containing your categories or labels"
                )
            
            with col3:
                feature_type = st.selectbox(
                    "Analysis Depth",
                    ["Lexical", "Semantic", "Syntactic", "Pragmatic", "Hybrid"],
                    help="Choose the depth of text analysis. Hybrid combines all features for best performance."
                )
            
            # SMOTE Configuration
            with st.expander("SMOTE Configuration"):
                col1, col2 = st.columns(2)
                with col1:
                    enable_smote = st.checkbox("Enable SMOTE Oversampling", value=True,
                                             help="Balance class distribution using SMOTE for better accuracy")
                with col2:
                    smote_k_neighbors = st.slider("SMOTE k-Neighbors", 
                                                min_value=1, max_value=10, value=5,
                                                help="Number of nearest neighbors for SMOTE algorithm")
            
            st.session_state.config = {
                'text_col': text_col,
                'target_col': target_col,
                'feature_type': feature_type,
                'enable_smote': enable_smote,
                'smote_k_neighbors': smote_k_neighbors
            }
            
            # Advanced Options
            with st.expander("Advanced Configuration"):
                col1, col2 = st.columns(2)
                with col1:
                    enable_cross_validation = st.checkbox("Enable Cross-Validation", value=True)
                    enable_hyperparameter_tuning = st.checkbox("Enable Hyperparameter Tuning", value=True)
                with col2:
                    min_samples_per_class = st.number_input("Minimum Samples per Class", 
                                                           min_value=10, value=50, 
                                                           help="Ensure sufficient data for reliable analysis")
            
            # Start Analysis Button
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                if st.button("Launch Advanced AI Analysis", use_container_width=True):
                    st.session_state.analyze_clicked = True

            # Dataset Overview
            st.markdown('<div class="section-header">Dataset Overview</div>', unsafe_allow_html=True)
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.markdown(f"""
                <div class="metric-card">
                    <div class="metric-value">{df.shape[0]}</div>
                    <div class="metric-label">Documents</div>
                </div>
                """, unsafe_allow_html=True)
            with col2:
                st.markdown(f"""
                <div class="metric-card">
                    <div class="metric-value">{df.shape[1]}</div>
                    <div class="metric-label">Features</div>
                </div>
                """, unsafe_allow_html=True)
            with col3:
                missing_vals = df.isnull().sum().sum()
                st.markdown(f"""
                <div class="metric-card">
                    <div class="metric-value">{missing_vals}</div>
                    <div class="metric-label">Missing Values</div>
                </div>
                """, unsafe_allow_html=True)
            with col4:
                unique_classes = df[target_col].nunique() if target_col in df.columns else 0
                st.markdown(f"""
                <div class="metric-card">
                    <div class="metric-value">{unique_classes}</div>
                    <div class="metric-label">Categories</div>
                </div>
                """, unsafe_allow_html=True)

            # Data Preview and Statistics
            col1, col2 = st.columns(2)
            with col1:
                with st.expander("Data Preview", expanded=True):
                    st.dataframe(df.head(10), use_container_width=True)
            with col2:
                with st.expander("Data Statistics"):
                    if text_col in df.columns:
                        text_stats = df[text_col].astype(str).apply(len)
                        st.write(f"Average text length: {text_stats.mean():.1f} characters")
                        st.write(f"Shortest text: {text_stats.min()} characters")
                        st.write(f"Longest text: {text_stats.max()} characters")
                        
                    if target_col in df.columns:
                        st.write("Class Distribution:")
                        class_dist = df[target_col].value_counts()
                        st.dataframe(class_dist, use_container_width=True)
                
        except Exception as e:
            st.error(f"Error reading file: {str(e)}")
    else:
        # Features Section
        st.markdown('<div class="section-header">Advanced Analytical Capabilities</div>', unsafe_allow_html=True)
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.markdown("""
            <div class="feature-card">
                <div class="feature-icon">A</div>
                <div class="card-header">Lexical Analysis</div>
                <p class="feature-description">
                    Advanced word-level analysis with sophisticated preprocessing, lemmatization, 
                    and vocabulary pattern recognition for comprehensive text understanding.
                </p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown("""
            <div class="feature-card">
                <div class="feature-icon">B</div>
                <div class="card-header">Semantic Analysis</div>
                <p class="feature-description">
                    Deep semantic understanding with sentiment analysis, readability scoring, 
                    and contextual meaning extraction for nuanced fact interpretation.
                </p>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            st.markdown("""
            <div class="feature-card">
                <div class="feature-icon">C</div>
                <div class="card-header">Syntactic Analysis</div>
                <p class="feature-description">
                    Structural pattern recognition with POS tagging, grammar analysis, 
                    and syntactic feature extraction for comprehensive text deconstruction.
                </p>
            </div>
            """, unsafe_allow_html=True)
        
        with col4:
            st.markdown("""
            <div class="feature-card">
                <div class="feature-icon">D</div>
                <div class="card-header">Pragmatic Analysis</div>
                <p class="feature-description">
                    Contextual and intent analysis with fact verification indicators, 
                    source credibility assessment, and persuasive element detection.
                </p>
            </div>
            """, unsafe_allow_html=True)

        # Use Cases
        st.markdown('<div class="section-header">Industry Applications</div>', unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            <div class="result-card">
                <h4 style="color: var(--primary-text); margin-bottom: 12px;">Fact Verification</h4>
                <p style="color: var(--primary-subtle); margin: 0;">
                    Automatically verify claims, detect misinformation, and validate factual statements 
                    across news articles, research papers, and public discourse.
                </p>
            </div>
            """, unsafe_allow_html=True)
            
            st.markdown("""
            <div class="result-card">
                <h4 style="color: var(--primary-text); margin-bottom: 12px;">Research Analysis</h4>
                <p style="color: var(--primary-subtle); margin: 0;">
                    Analyze academic papers, research findings, and scientific literature for 
                    evidence quality, methodological rigor, and conclusion validity.
                </p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown("""
            <div class="result-card">
                <h4 style="color: var(--primary-text); margin-bottom: 12px;">Business Intelligence</h4>
                <p style="color: var(--primary-subtle); margin: 0;">
                    Extract actionable insights from business reports, market analyses, 
                    and competitive intelligence for strategic decision-making.
                </p>
            </div>
            """, unsafe_allow_html=True)
            
            st.markdown("""
            <div class="result-card">
                <h4 style="color: var(--primary-text); margin-bottom: 12px;">Compliance Monitoring</h4>
                <p style="color: var(--primary-subtle); margin: 0;">
                    Monitor regulatory documents, compliance reports, and policy statements 
                    for adherence to standards and identification of potential issues.
                </p>
            </div>
            """, unsafe_allow_html=True)
    
    # Analysis results
    if st.session_state.get('analyze_clicked', False) and st.session_state.get('file_uploaded', False):
        perform_advanced_analysis(st.session_state.df, st.session_state.config)

# ============================
# Enhanced Analysis Function
# ============================
def perform_advanced_analysis(df, config):
    """Perform advanced analysis with professional styling"""
    st.markdown('<div class="section-header">Advanced Analysis Results</div>', unsafe_allow_html=True)
    
    # Data validation
    if config['text_col'] not in df.columns or config['target_col'] not in df.columns:
        st.error("Selected columns not found in dataset.")
        return

    # Enhanced data preprocessing
    with st.spinner("Performing advanced data preprocessing..."):
        # Handle missing values
        if df[config['text_col']].isnull().any():
            df[config['text_col']] = df[config['text_col']].fillna('')

        if df[config['target_col']].isnull().any():
            st.error("Target column contains missing values.")
            return

        if len(df[config['target_col']].unique()) < 2:
            st.error("Target column must have at least 2 unique classes.")
            return

        # Check for sufficient samples per class
        class_counts = df[config['target_col']].value_counts()
        if class_counts.min() < 10:
            st.warning(f"Some classes have very few samples (minimum: {class_counts.min()}). Results may be less reliable.")

    # Advanced feature extraction
    with st.spinner("Extracting advanced features from text data..."):
        extractor = AdvancedFeatureExtractor()
        X = df[config['text_col']].astype(str)
        y = df[config['target_col']]

        # Select feature extraction method
        if config['feature_type'] == "Lexical":
            X_features = extractor.extract_lexical_features(X)
            st.info("Using advanced lexical features with TF-IDF and n-grams")
        elif config['feature_type'] == "Semantic":
            X_features = extractor.extract_semantic_features(X)
            st.info("Using comprehensive semantic features with sentiment and readability analysis")
        elif config['feature_type'] == "Syntactic":
            X_features = extractor.extract_syntactic_features(X)
            st.info("Using syntactic features with POS patterns and structural analysis")
        elif config['feature_type'] == "Pragmatic":
            X_features = extractor.extract_pragmatic_features(X)
            st.info("Using pragmatic features for fact verification and contextual analysis")
        else:  # Hybrid
            X_features = extractor.extract_hybrid_features(X)
            st.info("Using hybrid features combining all analysis types for maximum performance")

    st.success("Feature extraction completed successfully!")

    # Enhanced model training with SMOTE
    with st.spinner("Training advanced AI models with optimized parameters..."):
        trainer = AdvancedModelTrainer()
        results, label_encoder = trainer.train_and_evaluate(X_features, y, enable_smote=config['enable_smote'])

    # Display enhanced results
    successful_models = {k: v for k, v in results.items() if 'error' not in v}

    if successful_models:
        # Model Performance Overview
        st.markdown("#### Model Performance Overview")
        
        cols = st.columns(len(successful_models))
        for idx, (model_name, result) in enumerate(successful_models.items()):
            with cols[idx]:
                accuracy = result['accuracy']
                cv_mean = result.get('cv_mean', accuracy)
                cv_std = result.get('cv_std', 0)
                
                st.markdown(f"""
                <div class="model-card">
                    <div class="card-header">{model_name}</div>
                    <div class="model-accuracy">{accuracy:.1%}</div>
                    <div style="color: var(--primary-subtle); font-size: 14px; margin-bottom: 12px;">
                        Precision: {result['precision']:.3f}<br>
                        Recall: {result['recall']:.3f}<br>
                        CV Score: {cv_mean:.3f} ± {cv_std:.3f}
                    </div>
                    <div class="progress-container">
                        <div class="progress-fill" style="width: {accuracy*100}%"></div>
                    </div>
                    <div style="color: var(--primary-blue); font-size: 12px; font-weight: 600; margin-top: 8px;">
                        F1-Score: {result['f1_score']:.3f}
                    </div>
                </div>
                """, unsafe_allow_html=True)

        # Performance Dashboard
        st.markdown("#### Performance Dashboard")
        viz = AdvancedVisualizer()
        dashboard_fig = viz.create_performance_dashboard(successful_models)
        st.pyplot(dashboard_fig)

        # Best Model Recommendation with Enhanced Analysis
        best_model = max(successful_models.items(), key=lambda x: x[1]['accuracy'])
        best_result = best_model[1]
        
        smote_status = "✅ Enabled" if best_result.get('smote_applied', False) else "❌ Disabled"
        
        st.markdown(f"""
        <div class="result-card success-card">
            <h3 style="color: var(--primary-success); margin-bottom: 1rem; display: flex; align-items: center; gap: 12px;">
                Recommended AI Model
            </h3>
            <p style="color: var(--primary-text); font-size: 1.1rem; margin-bottom: 0.5rem; line-height: 1.6;">
                <strong>{best_model[0]}</strong> achieved the highest accuracy of
                <strong style="color: var(--primary-success);">{best_result['accuracy']:.1%}</strong>
                with cross-validation score of <strong>{best_result.get('cv_mean', 0):.3f}</strong>.
            </p>
            <p style="color: var(--primary-subtle); margin: 0; line-height: 1.6;">
                This model demonstrates superior performance in analyzing your data patterns 
                and provides reliable classification across all {best_result['n_classes']} categories 
                with {best_result['test_size']} test samples. SMOTE Oversampling: {smote_status}
            </p>
        </div>
        """, unsafe_allow_html=True)

        # Confusion Matrix
        st.markdown("#### Confusion Matrix Analysis")
        confusion_fig = viz.create_confusion_matrix(successful_models, best_model[0], label_encoder)
        if confusion_fig:
            st.pyplot(confusion_fig)

        # Detailed Classification Report
        st.markdown("#### Detailed Classification Report")
        if best_model[0] in successful_models:
            result = successful_models[best_model[0]]
            report = classification_report(result['true_labels'], result['predictions'], 
                                         target_names=label_encoder.classes_, output_dict=True)
            report_df = pd.DataFrame(report).transpose()
            st.dataframe(report_df.style.format("{:.3f}").background_gradient(cmap='Blues'), 
                        use_container_width=True)

        # Feature Importance (if available)
        if best_result.get('feature_importance') is not None:
            st.markdown("#### Feature Importance Analysis")
            importance_df = pd.DataFrame({
                'feature': range(len(best_result['feature_importance'])),
                'importance': best_result['feature_importance']
            }).sort_values('importance', ascending=False).head(15)
            
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.barh(importance_df['feature'].astype(str), importance_df['importance'])
            ax.set_xlabel('Feature Importance')
            ax.set_title('Top 15 Most Important Features')
            plt.tight_layout()
            st.pyplot(fig)

    else:
        st.error("No models were successfully trained. Please check your data and try again.")

if __name__ == "__main__":
    main()
